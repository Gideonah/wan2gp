# ═══════════════════════════════════════════════════════════════════════════════
# Wan2GP API Server Dockerfile
# 
# This Dockerfile builds a container optimized for serverless API deployment
# on platforms like Vast.ai, RunPod, or Modal.
#
# Build:
#   docker build -f Dockerfile.api -t wan2gp-api .
#
# Run:
#   docker run --gpus all -p 8000:8000 \
#     -e WAN2GP_MODEL_TYPE=t2v \
#     -e WAN2GP_PROFILE=5 \
#     wan2gp-api
#
# Build Args:
#   CUDA_ARCHITECTURES: GPU architectures to compile for (default: "8.0;8.6")
#     Common values:
#       7.5 - RTX 2060, 2070, 2080
#       8.0 - A100 (Ampere datacenter)
#       8.6 - RTX 3060, 3070, 3080, 3090
#       8.9 - RTX 4070, 4080, 4090 (Ada Lovelace)
#       9.0 - H100 (Hopper datacenter)
#
# ═══════════════════════════════════════════════════════════════════════════════

FROM nvidia/cuda:12.4.1-cudnn-devel-ubuntu22.04

ARG CUDA_ARCHITECTURES="8.0;8.6"

ENV DEBIAN_FRONTEND=noninteractive

# Install system dependencies
RUN apt update && \
    apt install -y \
    python3 python3-pip git wget curl cmake ninja-build \
    libgl1 libglib2.0-0 ffmpeg && \
    apt clean

WORKDIR /workspace

# Copy and install requirements
COPY requirements.txt .
RUN pip install --upgrade pip setuptools wheel
RUN pip install -r requirements.txt

# Install PyTorch with CUDA support
RUN pip install --extra-index-url https://download.pytorch.org/whl/cu124 \
    torch==2.6.0+cu124 torchvision==0.21.0+cu124

# Install FastAPI and Uvicorn for API server
RUN pip install fastapi uvicorn python-multipart

# Install SageAttention from git (patch GPU detection for build without GPU)
ENV TORCH_CUDA_ARCH_LIST="${CUDA_ARCHITECTURES}"
ENV FORCE_CUDA="1"
ENV MAX_JOBS="1"

COPY <<EOF /tmp/patch_setup.py
import os
with open('setup.py', 'r') as f:
    content = f.read()

arch_list = os.environ.get('TORCH_CUDA_ARCH_LIST')
arch_set = '{' + ', '.join([f'"{arch}"' for arch in arch_list.split(';')]) + '}'

old_section = '''compute_capabilities = set()
device_count = torch.cuda.device_count()
for i in range(device_count):
    major, minor = torch.cuda.get_device_capability(i)
    if major < 8:
        warnings.warn(f"skipping GPU {i} with compute capability {major}.{minor}")
        continue
    compute_capabilities.add(f"{major}.{minor}")'''

new_section = 'compute_capabilities = ' + arch_set + '''
print(f"Manually set compute capabilities: {compute_capabilities}")'''

content = content.replace(old_section, new_section)

with open('setup.py', 'w') as f:
    f.write(content)
EOF

RUN git clone https://github.com/thu-ml/SageAttention.git /tmp/sageattention && \
    cd /tmp/sageattention && \
    python3 /tmp/patch_setup.py && \
    pip install --no-build-isolation .

# Create user
RUN useradd -u 1000 -ms /bin/bash user
RUN chown -R user:user /workspace
RUN mkdir -p /home/user/.cache && chown -R user:user /home/user/.cache

# Create outputs directory
RUN mkdir -p /workspace/outputs && chown -R user:user /workspace/outputs

# Copy entrypoints
COPY entrypoint.sh /workspace/entrypoint.sh
COPY entrypoint_api.sh /workspace/entrypoint_api.sh
RUN chmod +x /workspace/entrypoint.sh /workspace/entrypoint_api.sh

# Copy application code
COPY --chown=user:user . /workspace/

# Environment variables
ENV WAN2GP_MODEL_TYPE="t2v"
ENV WAN2GP_PROFILE="5"
ENV WAN2GP_PORT="8000"
ENV WAN2GP_OUTPUT_DIR="/workspace/outputs"

# Expose API port
EXPOSE 8000

# Use API entrypoint by default
ENTRYPOINT ["/workspace/entrypoint_api.sh"]

