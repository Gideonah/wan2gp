# ═══════════════════════════════════════════════════════════════════════════════
# Wan2GP for Vast.ai Serverless with Pre-Baked Model Weights
# 
# This Dockerfile creates an image optimized for Vast.ai serverless deployment:
#   - Model weights baked in (no cold start downloads)
#   - PyWorker integration for Vast's serverless routing
#   - Log-based readiness detection
#   - Benchmarking support for autoscaling
#
# BUILD PROCESS (run on a GPU machine for SageAttention):
#   docker build -f Dockerfile.vastai -t YOUR_REGISTRY/wan2gp-vastai:latest .
#   docker push YOUR_REGISTRY/wan2gp-vastai:latest
#
# VAST.AI TEMPLATE SETTINGS:
#   Image: YOUR_REGISTRY/wan2gp-vastai:latest
#   Docker Options: --gpus all
#   On-Start Script: /workspace/start_pyworker.sh
#
# ═══════════════════════════════════════════════════════════════════════════════

FROM nvidia/cuda:12.4.1-cudnn-devel-ubuntu22.04

ARG CUDA_ARCHITECTURES="8.0;8.6;8.9"

ENV DEBIAN_FRONTEND=noninteractive
ENV PYTHONUNBUFFERED=1

# System dependencies
RUN apt update && \
    apt install -y \
    python3 python3-pip git wget curl cmake ninja-build \
    libgl1 libglib2.0-0 ffmpeg && \
    apt clean && rm -rf /var/lib/apt/lists/*

WORKDIR /workspace

# ─────────────────────────────────────────────────────────────────────────────
# Python Dependencies
# ─────────────────────────────────────────────────────────────────────────────

COPY requirements.txt .
RUN pip install --upgrade pip setuptools wheel
RUN pip install -r requirements.txt

# PyTorch with CUDA (torch, torchvision, torchaudio)
RUN pip install --extra-index-url https://download.pytorch.org/whl/cu124 \
    torch==2.6.0+cu124 torchvision==0.21.0+cu124 torchaudio==2.6.0+cu124

# FastAPI for API server
RUN pip install fastapi uvicorn python-multipart

# Vast.ai PyWorker SDK
RUN pip install vastai

# NLTK for benchmark text generation
RUN pip install nltk && \
    python3 -c "import nltk; nltk.download('words', quiet=True)"

# ─────────────────────────────────────────────────────────────────────────────
# SageAttention (compile for target GPU architectures)
# ─────────────────────────────────────────────────────────────────────────────

ENV TORCH_CUDA_ARCH_LIST="${CUDA_ARCHITECTURES}"
ENV FORCE_CUDA="1"
ENV MAX_JOBS="1"

COPY <<EOF /tmp/patch_setup.py
import os
with open('setup.py', 'r') as f:
    content = f.read()

arch_list = os.environ.get('TORCH_CUDA_ARCH_LIST')
arch_set = '{' + ', '.join([f'"{arch}"' for arch in arch_list.split(';')]) + '}'

old_section = '''compute_capabilities = set()
device_count = torch.cuda.device_count()
for i in range(device_count):
    major, minor = torch.cuda.get_device_capability(i)
    if major < 8:
        warnings.warn(f"skipping GPU {i} with compute capability {major}.{minor}")
        continue
    compute_capabilities.add(f"{major}.{minor}")'''

new_section = 'compute_capabilities = ' + arch_set + '''
print(f"Manually set compute capabilities: {compute_capabilities}")'''

content = content.replace(old_section, new_section)

with open('setup.py', 'w') as f:
    f.write(content)
EOF

RUN git clone https://github.com/thu-ml/SageAttention.git /tmp/sageattention && \
    cd /tmp/sageattention && \
    python3 /tmp/patch_setup.py && \
    pip install --no-build-isolation . && \
    rm -rf /tmp/sageattention || echo "⚠️  SageAttention skipped (no CUDA during build)"

# ─────────────────────────────────────────────────────────────────────────────
# Application Code
# ─────────────────────────────────────────────────────────────────────────────

COPY . /workspace/

# Create directories
RUN mkdir -p /workspace/outputs /workspace/ckpts /var/log/wan2gp

# Make scripts executable
RUN chmod +x /workspace/start_pyworker.sh /workspace/entrypoint_api.sh 2>/dev/null || true

# ─────────────────────────────────────────────────────────────────────────────
# Download Model Weights (THIS IS THE KEY STEP)
# ─────────────────────────────────────────────────────────────────────────────

ENV HF_HOME=/workspace/.cache/huggingface

# Download ONLY the files needed for t2v (text-to-video)
# The full repo is ~50GB+, but t2v only needs ~12GB
# To use a different model, modify download_model.py
COPY download_model.py /tmp/download_model.py
RUN python3 /tmp/download_model.py && rm /tmp/download_model.py

# ─────────────────────────────────────────────────────────────────────────────
# Runtime Configuration
# ─────────────────────────────────────────────────────────────────────────────

# Model configuration
ENV WAN2GP_MODEL_TYPE="t2v"
ENV WAN2GP_PROFILE="5"
ENV WAN2GP_OUTPUT_DIR="/workspace/outputs"

# Backend API server port (internal)
ENV MODEL_SERVER_PORT="8000"

# Log file for PyWorker monitoring
ENV WAN2GP_LOG_FILE="/var/log/wan2gp/server.log"

# Vast.ai will expose port 80 for the PyWorker
EXPOSE 80

# Healthcheck via the PyWorker proxy
HEALTHCHECK --interval=30s --timeout=10s --start-period=600s \
    CMD curl -f http://localhost:80/health || exit 1

# Start the PyWorker (which starts the backend API server)
CMD ["/workspace/start_pyworker.sh"]

