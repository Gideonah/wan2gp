# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Wan2GP for Vast.ai Serverless with Pre-Baked Model Weights
# 
# This Dockerfile creates an image optimized for Vast.ai serverless deployment:
#   - Model weights baked in (no cold start downloads)
#   - PyWorker integration for Vast's serverless routing
#   - Log-based readiness detection
#   - Benchmarking support for autoscaling
#
# BUILD PROCESS (run on a GPU machine for SageAttention):
#   docker build -f Dockerfile.vastai -t YOUR_REGISTRY/wan2gp-vastai:latest .
#   docker push YOUR_REGISTRY/wan2gp-vastai:latest
#
# VAST.AI TEMPLATE SETTINGS:
#   Image: YOUR_REGISTRY/wan2gp-vastai:latest
#   Docker Options: --gpus all
#   On-Start Script: /workspace/start_pyworker.sh
#
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

FROM nvidia/cuda:12.4.1-cudnn-devel-ubuntu22.04

ARG CUDA_ARCHITECTURES="8.0;8.6;8.9"

ENV DEBIAN_FRONTEND=noninteractive
ENV PYTHONUNBUFFERED=1

# System dependencies
RUN apt update && \
    apt install -y \
    python3 python3-pip git wget curl cmake ninja-build \
    libgl1 libglib2.0-0 ffmpeg && \
    apt clean && rm -rf /var/lib/apt/lists/*

WORKDIR /workspace

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Python Dependencies
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

COPY requirements.txt .
RUN pip install --upgrade pip setuptools wheel
RUN pip install -r requirements.txt

# PyTorch with CUDA
RUN pip install --extra-index-url https://download.pytorch.org/whl/cu124 \
    torch==2.6.0+cu124 torchvision==0.21.0+cu124

# FastAPI for API server
RUN pip install fastapi uvicorn python-multipart

# Vast.ai PyWorker SDK
RUN pip install vastai

# NLTK for benchmark text generation
RUN pip install nltk && \
    python3 -c "import nltk; nltk.download('words', quiet=True)"

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# SageAttention (compile for target GPU architectures)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

ENV TORCH_CUDA_ARCH_LIST="${CUDA_ARCHITECTURES}"
ENV FORCE_CUDA="1"
ENV MAX_JOBS="1"

COPY <<EOF /tmp/patch_setup.py
import os
with open('setup.py', 'r') as f:
    content = f.read()

arch_list = os.environ.get('TORCH_CUDA_ARCH_LIST')
arch_set = '{' + ', '.join([f'"{arch}"' for arch in arch_list.split(';')]) + '}'

old_section = '''compute_capabilities = set()
device_count = torch.cuda.device_count()
for i in range(device_count):
    major, minor = torch.cuda.get_device_capability(i)
    if major < 8:
        warnings.warn(f"skipping GPU {i} with compute capability {major}.{minor}")
        continue
    compute_capabilities.add(f"{major}.{minor}")'''

new_section = 'compute_capabilities = ' + arch_set + '''
print(f"Manually set compute capabilities: {compute_capabilities}")'''

content = content.replace(old_section, new_section)

with open('setup.py', 'w') as f:
    f.write(content)
EOF

RUN git clone https://github.com/thu-ml/SageAttention.git /tmp/sageattention && \
    cd /tmp/sageattention && \
    python3 /tmp/patch_setup.py && \
    pip install --no-build-isolation . && \
    rm -rf /tmp/sageattention || echo "âš ï¸  SageAttention skipped (no CUDA during build)"

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Application Code
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

COPY . /workspace/

# Create directories
RUN mkdir -p /workspace/outputs /workspace/ckpts /var/log/wan2gp

# Make scripts executable
RUN chmod +x /workspace/start_pyworker.sh /workspace/entrypoint_api.sh 2>/dev/null || true

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Download Model Weights (THIS IS THE KEY STEP)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

ENV HF_HOME=/workspace/.cache/huggingface

# Download ONLY the files needed for t2v (text-to-video)
# The full repo is ~50GB+, but t2v only needs ~14GB
#
# To use a different model (i2v, vace, etc.), modify T2V_PATTERNS below
#
RUN python3 << 'EOF'
import os
os.environ['HF_HOME'] = '/workspace/.cache/huggingface'

from huggingface_hub import snapshot_download

REPO_ID = "DeepBeepMeep/Wan2.1"
LOCAL_DIR = "/workspace/.cache/huggingface/hub/models--DeepBeepMeep--Wan2.1"

# Patterns for t2v model
# Note: Using quantized (int8) saves ~50% disk/memory
T2V_PATTERNS = [
    # â”€â”€â”€ T2V Transformer (choose ONE) â”€â”€â”€
    # "wan2.1_text2video_14B_mbf16.safetensors",              # ~14GB, best quality
    "wan2.1_text2video_14B_quanto_mbf16_int8.safetensors",    # ~7GB, quantized
    
    # â”€â”€â”€ CLIP Text Encoder â”€â”€â”€
    "xlm-roberta-large/models_clip_open-clip-xlm-roberta-large-vit-huge-14-bf16.safetensors",
    "xlm-roberta-large/sentencepiece.bpe.model",
    "xlm-roberta-large/special_tokens_map.json",
    "xlm-roberta-large/tokenizer.json",
    "xlm-roberta-large/tokenizer_config.json",
    
    # â”€â”€â”€ T5 Text Encoder â”€â”€â”€
    "umt5-xxl/special_tokens_map.json",
    "umt5-xxl/spiece.model",
    "umt5-xxl/tokenizer.json", 
    "umt5-xxl/tokenizer_config.json",
    "umt5-xxl-enc-quanto_int8.safetensors",
    
    # â”€â”€â”€ VAE â”€â”€â”€
    "Wan2.1_VAE.safetensors",
]

print("ğŸ“¥ Downloading Wan2.1 t2v model (quantized, ~12GB total)...")

snapshot_download(
    repo_id=REPO_ID,
    local_dir=LOCAL_DIR,
    allow_patterns=T2V_PATTERNS,
    local_dir_use_symlinks=False,
)

print("âœ… Wan2.1 t2v model downloaded!")
EOF

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Runtime Configuration
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# Model configuration
ENV WAN2GP_MODEL_TYPE="t2v"
ENV WAN2GP_PROFILE="5"
ENV WAN2GP_OUTPUT_DIR="/workspace/outputs"

# Backend API server port (internal)
ENV MODEL_SERVER_PORT="8000"

# Log file for PyWorker monitoring
ENV WAN2GP_LOG_FILE="/var/log/wan2gp/server.log"

# Vast.ai will expose port 80 for the PyWorker
EXPOSE 80

# Healthcheck via the PyWorker proxy
HEALTHCHECK --interval=30s --timeout=10s --start-period=600s \
    CMD curl -f http://localhost:80/health || exit 1

# Start the PyWorker (which starts the backend API server)
CMD ["/workspace/start_pyworker.sh"]

