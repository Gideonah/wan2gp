# ═══════════════════════════════════════════════════════════════════════════════
# LTX-2 Distilled Video Generation for Vast.ai Serverless
# 
# This Dockerfile creates an image optimized for Vast.ai deployment:
#   - LTX-2 Distilled model for fast image-to-video generation
#   - Models downloaded at runtime (smaller image)
#   - STANDARD Vast.ai PyWorker integration via start_server.sh
#   - Log-based readiness detection
#   - Benchmarking support for autoscaling
#
# BUILD PROCESS (run on a GPU machine for SageAttention):
#   docker build -f Dockerfile.vastai -t YOUR_REGISTRY/ltx2-vastai:latest .
#   docker push YOUR_REGISTRY/ltx2-vastai:latest
#
# VAST.AI TEMPLATE SETTINGS:
#   Image: YOUR_REGISTRY/ltx2-vastai:latest
#   Docker Options: --gpus all
#   On-Start Script: /workspace/start_vastai.sh
#   Environment Variables:
#     PYWORKER_REPO: https://github.com/YOUR_USER/YOUR_REPO (containing pyworker/worker.py)
#     PYWORKER_REF: main (optional, defaults to main)
#
# NOTE: First startup will download model weights (~20-25GB for LTX-2 distilled).
#       Mount a persistent volume to /workspace/.cache for faster restarts.
#
# ═══════════════════════════════════════════════════════════════════════════════

FROM nvidia/cuda:12.4.1-cudnn-devel-ubuntu22.04

ARG CUDA_ARCHITECTURES="8.0;8.6;8.9"

ENV DEBIAN_FRONTEND=noninteractive
ENV PYTHONUNBUFFERED=1

# System dependencies (minimal install)
RUN apt update && \
    apt install -y --no-install-recommends \
    python3 python3-pip git wget curl cmake ninja-build \
    libgl1 libglib2.0-0 ffmpeg ca-certificates && \
    apt clean && \
    rm -rf /var/lib/apt/lists/* /tmp/* /var/tmp/*

WORKDIR /workspace

# ─────────────────────────────────────────────────────────────────────────────
# Python Dependencies
# ─────────────────────────────────────────────────────────────────────────────

COPY requirements.txt .

RUN pip install --upgrade pip setuptools wheel

# Install PyTorch with CUDA support FIRST (required for packages with CUDA extensions)
RUN pip install --extra-index-url https://download.pytorch.org/whl/cu124 \
        torch==2.6.0+cu124 torchvision==0.21.0+cu124 torchaudio==2.6.0+cu124

# Install requirements (insightface needs PyTorch/CUDA already installed)
RUN pip install -r requirements.txt

# Install additional API dependencies
RUN pip install fastapi uvicorn python-multipart httpx

# # Install all Python dependencies in one layer with cache cleanup
# RUN pip install --upgrade pip setuptools wheel && \
#     pip install -r requirements.txt && \
#     pip install --extra-index-url https://download.pytorch.org/whl/cu124 \
#         torch==2.6.0+cu124 torchvision==0.21.0+cu124 torchaudio==2.6.0+cu124 && \
#     pip install fastapi uvicorn python-multipart httpx && \
#     python3 -c "import nltk; nltk.download('words', quiet=True)" && \
#     rm -rf /root/.cache/pip /tmp/* && \
#     find /usr/local -name "*.pyc" -delete && \
#     find /usr/local -name "__pycache__" -type d -exec rm -rf {} + 2>/dev/null || true

# ─────────────────────────────────────────────────────────────────────────────
# SageAttention (compile for target GPU architectures)
# ─────────────────────────────────────────────────────────────────────────────

ENV TORCH_CUDA_ARCH_LIST="${CUDA_ARCHITECTURES}"
ENV FORCE_CUDA="1"
ENV MAX_JOBS="1"

COPY <<EOF /tmp/patch_setup.py
import os
with open('setup.py', 'r') as f:
    content = f.read()

arch_list = os.environ.get('TORCH_CUDA_ARCH_LIST')
arch_set = '{' + ', '.join([f'"{arch}"' for arch in arch_list.split(';')]) + '}'

old_section = '''compute_capabilities = set()
device_count = torch.cuda.device_count()
for i in range(device_count):
    major, minor = torch.cuda.get_device_capability(i)
    if major < 8:
        warnings.warn(f"skipping GPU {i} with compute capability {major}.{minor}")
        continue
    compute_capabilities.add(f"{major}.{minor}")'''

new_section = 'compute_capabilities = ' + arch_set + '''
print(f"Manually set compute capabilities: {compute_capabilities}")'''

content = content.replace(old_section, new_section)

with open('setup.py', 'w') as f:
    f.write(content)
EOF

RUN git clone --depth 1 https://github.com/thu-ml/SageAttention.git /tmp/sageattention && \
    cd /tmp/sageattention && \
    python3 /tmp/patch_setup.py && \
    pip install --no-build-isolation . && \
    rm -rf /tmp/sageattention /tmp/patch_setup.py /root/.cache/* || \
    echo "⚠️  SageAttention skipped (no CUDA during build)"

# ─────────────────────────────────────────────────────────────────────────────
# Application Code
# ─────────────────────────────────────────────────────────────────────────────

COPY . /workspace/

# Create directories
RUN mkdir -p /workspace/outputs /workspace/ckpts /var/log/wan2gp

# Make scripts executable
RUN chmod +x /workspace/start_vastai.sh /workspace/start_pyworker.sh /workspace/entrypoint_api.sh 2>/dev/null || true

# ─────────────────────────────────────────────────────────────────────────────
# Runtime Configuration
# ─────────────────────────────────────────────────────────────────────────────

# HuggingFace cache (models downloaded at runtime)
ENV HF_HOME=/workspace/.cache/huggingface

# Model configuration (LTX-2 Distilled)
ENV WAN2GP_MODEL_TYPE="ltx2_distilled"
ENV WAN2GP_PROFILE="3"
ENV WAN2GP_OUTPUT_DIR="/workspace/outputs"

# Backend API server port (internal)
ENV MODEL_SERVER_PORT="8000"

# Log file for PyWorker monitoring
ENV WAN2GP_LOG_FILE="/var/log/wan2gp/server.log"

# PyWorker repo (set this in your Vast template, or override here)
# Points to the directory containing worker.py and requirements.txt
# ENV PYWORKER_REPO="https://github.com/YOUR_USER/YOUR_REPO"
# ENV PYWORKER_REF="main"

# Vast.ai will expose port 80 for the PyWorker
EXPOSE 80

# Healthcheck via the PyWorker proxy
HEALTHCHECK --interval=30s --timeout=10s --start-period=600s \
    CMD curl -f http://localhost:80/health || exit 1

# Start using standard Vast.ai approach:
# 1. Starts backend API server
# 2. Calls wget start_server.sh which installs vastai-sdk and runs worker.py from PYWORKER_REPO
CMD ["/workspace/start_vastai.sh"]

