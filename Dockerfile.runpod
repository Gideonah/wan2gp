# ═══════════════════════════════════════════════════════════════════════════════
# Wan2GP LTX-2 Video Generation for RunPod Serverless
#
# This Dockerfile bakes the LTX-2 Distilled model into the image for fast cold starts.
# Image size: ~25-30GB (includes ~15GB of model weights)
#
# BUILD (requires network access):
#   docker build -f Dockerfile.runpod -t YOUR_REGISTRY/ltx2-runpod:latest .
#   docker push YOUR_REGISTRY/ltx2-runpod:latest
#
# DEPLOY:
#   1. Go to RunPod Console → Serverless → New Endpoint
#   2. Select your pushed image
#   3. Configure GPU type (A100, RTX 4090, etc.)
#   4. Deploy - model is already included, no download needed!
#
# ═══════════════════════════════════════════════════════════════════════════════

# Use RunPod's PyTorch base image with CUDA support
FROM runpod/pytorch:2.4.0-py3.11-cuda12.4.1-devel-ubuntu22.04

# Prevent interactive prompts and ensure unbuffered output
ENV DEBIAN_FRONTEND=noninteractive
ENV PYTHONUNBUFFERED=1

# HuggingFace cache - set early so model downloads go to right place
ENV HF_HOME=/workspace/.cache/huggingface

# Install minimal system dependencies
RUN apt-get update && apt-get install -y --no-install-recommends \
    ffmpeg \
    libgl1 \
    libglib2.0-0 \
    && apt-get clean \
    && rm -rf /var/lib/apt/lists/* /tmp/* /var/tmp/*

# Set working directory for application code
WORKDIR /app

# Copy requirements first (for layer caching)
COPY requirements.txt /app/

# Install Python dependencies
RUN pip install --no-cache-dir -r requirements.txt && \
    pip install --no-cache-dir \
        runpod \
        fastapi \
        uvicorn \
        httpx \
        python-multipart \
        google-cloud-storage

# Upgrade PyTorch to 2.6.0 with CUDA 12.4 support
RUN pip install --no-cache-dir --extra-index-url https://download.pytorch.org/whl/cu124 \
    torch==2.6.0+cu124 torchvision==0.21.0+cu124 torchaudio==2.6.0+cu124

# Copy application code
COPY . /app/

# Copy handler to root (RunPod convention)
COPY handler.py /handler.py

# Create output and cache directories
RUN mkdir -p /workspace/outputs /workspace/.cache/huggingface

# ─────────────────────────────────────────────────────────────────────────────
# BAKE LTX-2 DISTILLED MODEL INTO IMAGE
# Downloads ~15GB of model weights during build
# ─────────────────────────────────────────────────────────────────────────────

# Download LTX-2 Distilled FP8 model (~15GB total)
# Using FP8 transformer to save ~9GB (10GB vs 19GB)
RUN python /app/download_ltx2_distilled.py --fp8

# Verify model files exist
RUN echo "Verifying model files..." && \
    ls -la /workspace/.cache/huggingface/hub/models--DeepBeepMeep--LTX-2/snapshots/*/ltx-2-19b-distilled-fp8_diffusion_model.safetensors && \
    echo "✅ LTX-2 Distilled FP8 model baked into image"

# ─────────────────────────────────────────────────────────────────────────────
# Runtime Configuration
# ─────────────────────────────────────────────────────────────────────────────

# Model configuration
ENV WAN2GP_MODEL_TYPE="ltx2_distilled"
ENV WAN2GP_PROFILE="3"
ENV WAN2GP_OUTPUT_DIR="/workspace/outputs"

# Skip preprocessing model downloads (not needed for I2V API)
ENV WAN2GP_SKIP_SHARED_DOWNLOADS="1"

# Application root for imports
ENV PYTHONPATH="/app"

# RunPod serverless entry point
CMD ["python", "-u", "/handler.py"]
